{
  "gradient_checkpointing": true,
  "torch_dtype": "bfloat16",
  "dataloader_num_workers": 4,
  "seed": 42,
  "report_to": "none",
  "per_device_train_batch_size": 16,
  "gradient_accumulation_steps": 2,
  "max_seq_length": 1024,
  "lora_r": 16,
  "learning_rate": 0.0001,
  "num_train_epochs": 1,
  "warmup_steps": 100,
  "logging_steps": 50,
  "save_steps": 1000,
  "eval_steps": 500,
  "mlm_probability": 0.2,
  "mask_token_type": "blank",
  "stop_after_n_steps": 2000,
  "model_name_or_path": "data/models/huggingfacetb-smollm3-3b-bi-init",
  "output_dir": "data/models/huggingfacetb-smollm3-3b-bi-mntp",
  "do_train": true,
  "do_eval": true,
  "eval_strategy": "steps",
  "data_collator_type": "default",
  "dataset_name": "wikimedia/wikipedia",
  "dataset_config_name": "20231101.de",
  "overwrite_output_dir": true,
  "per_device_eval_batch_size": 16
}